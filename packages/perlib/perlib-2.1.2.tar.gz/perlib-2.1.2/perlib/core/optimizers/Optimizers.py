import tensorflow.compat.v2 as tf
from keras import backend_config
from keras.optimizers.optimizer_v2 import optimizer_v2

# isort: off
from tensorflow.python.util.tf_export import keras_export

class optimizer:
    def __int__(self):
        self.Adam = self.Adam()
        self.SGD  = self.SGD()

    @keras_export(
        "keras.optimizers.legacy.Adam",
        v1=["keras.optimizers.Adam", "keras.optimizers.legacy.Adam"],
    )
    class Adam(optimizer_v2.OptimizerV2):

        _HAS_AGGREGATE_GRAD = True

        def __init__(
            self,
            learning_rate=0.001,
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-7,
            amsgrad=False,
            name="Adam",
            **kwargs
        ):
            super().__init__(name, **kwargs)
            self._set_hyper("learning_rate", kwargs.get("lr", learning_rate))
            self._set_hyper("decay", self._initial_decay)
            self._set_hyper("beta_1", beta_1)
            self._set_hyper("beta_2", beta_2)
            self.epsilon = epsilon or backend_config.epsilon()
            self.amsgrad = amsgrad

        def _create_slots(self, var_list):
            # Create slots for the first and second moments.
            # Separate for-loops to respect the ordering of slot variables from v1.
            for var in var_list:
                self.add_slot(var, "m")
            for var in var_list:
                self.add_slot(var, "v")
            if self.amsgrad:
                for var in var_list:
                    self.add_slot(var, "vhat")

        def _prepare_local(self, var_device, var_dtype, apply_state):
            super()._prepare_local(var_device, var_dtype, apply_state)

            local_step = tf.cast(self.iterations + 1, var_dtype)
            beta_1_t = tf.identity(self._get_hyper("beta_1", var_dtype))
            beta_2_t = tf.identity(self._get_hyper("beta_2", var_dtype))
            beta_1_power = tf.pow(beta_1_t, local_step)
            beta_2_power = tf.pow(beta_2_t, local_step)
            lr = apply_state[(var_device, var_dtype)]["lr_t"] * (
                tf.sqrt(1 - beta_2_power) / (1 - beta_1_power)
            )
            apply_state[(var_device, var_dtype)].update(
                dict(
                    lr=lr,
                    epsilon=tf.convert_to_tensor(self.epsilon, var_dtype),
                    beta_1_t=beta_1_t,
                    beta_1_power=beta_1_power,
                    one_minus_beta_1_t=1 - beta_1_t,
                    beta_2_t=beta_2_t,
                    beta_2_power=beta_2_power,
                    one_minus_beta_2_t=1 - beta_2_t,
                )
            )

        def set_weights(self, weights):
            params = self.weights
            # If the weights are generated by Keras V1 optimizer, it includes vhats
            # even without amsgrad, i.e, V1 optimizer has 3x + 1 variables, while V2
            # optimizer has 2x + 1 variables. Filter vhats out for compatibility.
            num_vars = int((len(params) - 1) / 2)
            if len(weights) == 3 * num_vars + 1:
                weights = weights[: len(params)]
            super().set_weights(weights)

        def _resource_apply_dense(self, grad, var, apply_state=None):
            var_device, var_dtype = var.device, var.dtype.base_dtype
            coefficients = (apply_state or {}).get(
                (var_device, var_dtype)
            ) or self._fallback_apply_state(var_device, var_dtype)

            m = self.get_slot(var, "m")
            v = self.get_slot(var, "v")

            if not self.amsgrad:
                return tf.raw_ops.ResourceApplyAdam(
                    var=var.handle,
                    m=m.handle,
                    v=v.handle,
                    beta1_power=coefficients["beta_1_power"],
                    beta2_power=coefficients["beta_2_power"],
                    lr=coefficients["lr_t"],
                    beta1=coefficients["beta_1_t"],
                    beta2=coefficients["beta_2_t"],
                    epsilon=coefficients["epsilon"],
                    grad=grad,
                    use_locking=self._use_locking,
                )
            else:
                vhat = self.get_slot(var, "vhat")
                return tf.raw_ops.ResourceApplyAdamWithAmsgrad(
                    var=var.handle,
                    m=m.handle,
                    v=v.handle,
                    vhat=vhat.handle,
                    beta1_power=coefficients["beta_1_power"],
                    beta2_power=coefficients["beta_2_power"],
                    lr=coefficients["lr_t"],
                    beta1=coefficients["beta_1_t"],
                    beta2=coefficients["beta_2_t"],
                    epsilon=coefficients["epsilon"],
                    grad=grad,
                    use_locking=self._use_locking,
                )

        def _resource_apply_sparse(self, grad, var, indices, apply_state=None):
            var_device, var_dtype = var.device, var.dtype.base_dtype
            coefficients = (apply_state or {}).get(
                (var_device, var_dtype)
            ) or self._fallback_apply_state(var_device, var_dtype)

            # m_t = beta1 * m + (1 - beta1) * g_t
            m = self.get_slot(var, "m")
            m_scaled_g_values = grad * coefficients["one_minus_beta_1_t"]
            m_t = tf.compat.v1.assign(
                m, m * coefficients["beta_1_t"], use_locking=self._use_locking
            )
            with tf.control_dependencies([m_t]):
                m_t = self._resource_scatter_add(m, indices, m_scaled_g_values)

            # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)
            v = self.get_slot(var, "v")
            v_scaled_g_values = (grad * grad) * coefficients["one_minus_beta_2_t"]
            v_t = tf.compat.v1.assign(
                v, v * coefficients["beta_2_t"], use_locking=self._use_locking
            )
            with tf.control_dependencies([v_t]):
                v_t = self._resource_scatter_add(v, indices, v_scaled_g_values)

            if not self.amsgrad:
                v_sqrt = tf.sqrt(v_t)
                var_update = tf.compat.v1.assign_sub(
                    var,
                    coefficients["lr"] * m_t / (v_sqrt + coefficients["epsilon"]),
                    use_locking=self._use_locking,
                )
                return tf.group(*[var_update, m_t, v_t])
            else:
                v_hat = self.get_slot(var, "vhat")
                v_hat_t = tf.maximum(v_hat, v_t)
                with tf.control_dependencies([v_hat_t]):
                    v_hat_t = tf.compat.v1.assign(
                        v_hat, v_hat_t, use_locking=self._use_locking
                    )
                v_hat_sqrt = tf.sqrt(v_hat_t)
                var_update = tf.compat.v1.assign_sub(
                    var,
                    coefficients["lr"]
                    * m_t
                    / (v_hat_sqrt + coefficients["epsilon"]),
                    use_locking=self._use_locking,
                )
                return tf.group(*[var_update, m_t, v_t, v_hat_t])

        def get_config(self):
            config = super().get_config()
            config.update(
                {
                    "learning_rate": self._serialize_hyperparameter(
                        "learning_rate"
                    ),
                    "decay": self._initial_decay,
                    "beta_1": self._serialize_hyperparameter("beta_1"),
                    "beta_2": self._serialize_hyperparameter("beta_2"),
                    "epsilon": self.epsilon,
                    "amsgrad": self.amsgrad,
                }
            )
            return config


    @keras_export(
        "keras.optimizers.legacy.SGD",
        v1=["keras.optimizers.SGD", "keras.optimizers.legacy.SGD"],
    )
    class SGD(optimizer_v2.OptimizerV2):

        _HAS_AGGREGATE_GRAD = True

        def __init__(
            self,
            learning_rate=0.01,
            momentum=0.0,
            nesterov=False,
            name="SGD",
            **kwargs,
        ):
            super().__init__(name, **kwargs)
            self._set_hyper("learning_rate", kwargs.get("lr", learning_rate))
            self._set_hyper("decay", self._initial_decay)

            self._momentum = False
            if (
                isinstance(momentum, tf.Tensor)
                or callable(momentum)
                or momentum > 0
            ):
                self._momentum = True
            if isinstance(momentum, (int, float)) and (
                momentum < 0 or momentum > 1
            ):
                raise ValueError(
                    "`momentum` must be between [0, 1]. Received: "
                    f"momentum={momentum} (of type {type(momentum)})."
                )
            self._set_hyper("momentum", momentum)

            self.nesterov = nesterov

        def _create_slots(self, var_list):
            if self._momentum:
                for var in var_list:
                    self.add_slot(var, "momentum")

        def _prepare_local(self, var_device, var_dtype, apply_state):
            super()._prepare_local(var_device, var_dtype, apply_state)
            apply_state[(var_device, var_dtype)]["momentum"] = tf.identity(
                self._get_hyper("momentum", var_dtype)
            )

        def _resource_apply_dense(self, grad, var, apply_state=None):
            var_device, var_dtype = var.device, var.dtype.base_dtype
            coefficients = (apply_state or {}).get(
                (var_device, var_dtype)
            ) or self._fallback_apply_state(var_device, var_dtype)

            if self._momentum:
                momentum_var = self.get_slot(var, "momentum")
                return tf.raw_ops.ResourceApplyKerasMomentum(
                    var=var.handle,
                    accum=momentum_var.handle,
                    lr=coefficients["lr_t"],
                    grad=grad,
                    momentum=coefficients["momentum"],
                    use_locking=self._use_locking,
                    use_nesterov=self.nesterov,
                )
            else:
                return tf.raw_ops.ResourceApplyGradientDescent(
                    var=var.handle,
                    alpha=coefficients["lr_t"],
                    delta=grad,
                    use_locking=self._use_locking,
                )

        def _resource_apply_sparse_duplicate_indices(
            self, grad, var, indices, **kwargs
        ):
            if self._momentum:
                return super()._resource_apply_sparse_duplicate_indices(
                    grad, var, indices, **kwargs
                )
            else:
                var_device, var_dtype = var.device, var.dtype.base_dtype
                coefficients = kwargs.get("apply_state", {}).get(
                    (var_device, var_dtype)
                ) or self._fallback_apply_state(var_device, var_dtype)

                return tf.raw_ops.ResourceScatterAdd(
                    resource=var.handle,
                    indices=indices,
                    updates=-grad * coefficients["lr_t"],
                )

        def _resource_apply_sparse(self, grad, var, indices, apply_state=None):
            # This method is only needed for momentum optimization.
            var_device, var_dtype = var.device, var.dtype.base_dtype
            coefficients = (apply_state or {}).get(
                (var_device, var_dtype)
            ) or self._fallback_apply_state(var_device, var_dtype)

            momentum_var = self.get_slot(var, "momentum")
            return tf.raw_ops.ResourceSparseApplyKerasMomentum(
                var=var.handle,
                accum=momentum_var.handle,
                lr=coefficients["lr_t"],
                grad=grad,
                indices=indices,
                momentum=coefficients["momentum"],
                use_locking=self._use_locking,
                use_nesterov=self.nesterov,
            )

        def get_config(self):
            config = super().get_config()
            config.update(
                {
                    "learning_rate": self._serialize_hyperparameter(
                        "learning_rate"
                    ),
                    "decay": self._initial_decay,
                    "momentum": self._serialize_hyperparameter("momentum"),
                    "nesterov": self.nesterov,
                }
            )
            return config
