from pytorch_forecasting import TemporalFusionTransformer as TFT

from ._seq2seq import Seq2SeqOutputDecoder
from .base import BaseCollateFn
from .base import PytorchForecastingEstimator
from .datasets import TimeseriesDataset


class TemporalFusionTransformer(PytorchForecastingEstimator):
    """Temporal Fusion Transformer for forecasting time series.

    Implementation of the article Temporal Fusion Transformers for
    Interpretable Multi-horizon Time Series Forecasting. The network
    outperforms DeepAR by Amazon by 36-69% in benchmarks.

    Parameters
    ----------
    group_ids : list of str
        List of column names identifying a time series. This means that the
        ``group_ids`` identify a sample together with the ``date``. If you
        have only one times eries, set this to the name of column that is
        constant.

    time_idx : str
        Time index column. This column is used to determine the sequence of
        samples.

    target : str
        Target column. Column containing the values to be predicted.

    max_prediction_length : int
        Maximum prediction/decoder length. Usually this this is defined by
        the difference between forecasting dates.

    max_encoder_length : int, default=None
        Maximum length to encode (also known as `input sequence length`). This
        is the maximum history length used by the time series dataset. If None,
        3 times the ``max_prediction_length`` is used.

    time_varying_known_reals : list of str, default=None
        List of continuous variables that change over time and are known in the
        future (e.g. price of a product, but not demand of a product). If None,
        every numeric column excluding ``target`` is used.

    time_varying_unknown_reals : list of str
        List of continuous variables that change over time and are not known in
        the future. You might want to include your ``target`` here. If None,
        only ``target`` is used.

    static_categoricals : list of str
        List of categorical variables that do not change over time (also known
        as `time independent variables`). You might want to include your
        ``group_ids`` here for the learning algorithm to distinguish between
        different time series. If None, only ``group_ids`` is used.

    criterion : class, default=None
        The uninitialized criterion (loss) used to optimize the module. If
        None, the :class:`.RMSE` is used.

    optimizer : class, default=None
        The uninitialized optimizer (update rule) used to optimize the
        module. if None, :class:`.Adam` optimizer is used.

    lr : float, default=1e-5
        Learning rate passed to the optimizer.

    max_epochs : int, default=10
        The number of epochs to train for each :meth:`fit` call. Note that you
        may keyboard-interrupt training at any time.

    batch_size : int, default=64
        Mini-batch size. If ``batch_size`` is -1, a single batch with
        all the data will be used during training and validation.

    callbacks: None, “disable”, or list of Callback instances, default=None
        Which callbacks to enable.

        - If callbacks=None, only use default callbacks which include:
            - `epoch_timer`: measures the duration of each epoch
            - `train_loss`: computes average of train batch losses
            - `valid_loss`: computes average of valid batch losses
            - `print_log`:  prints all of the above in nice format

        - If callbacks="disable":
            disable all callbacks, i.e. do not run any of the callbacks.

        - If callbacks is a list of callbacks:
            use those callbacks in addition to the default callbacks. Each
            callback should be an instance of skorch :class:`.Callback`.

    emb_dim : int, default=10
        Dimension of every embedding table

    hidden_size : int, default=16
        Size of the context vector

    hidden_continuous_size : int, default=8
        Hidden size for processing continuous variables

    lstm_layers : int, default=2
        Number of LSTM layers (2 is mostly optimal)

    dropout : float, default=0.1
        Dropout rate

    verbose : int, default=1
        This parameter controls how much print output is generated by
        the net and its callbacks. By setting this value to 0, e.g. the
        summary scores at the end of each epoch are no longer printed.
        This can be useful when running a hyperparameter search. The
        summary scores are always logged in the history attribute,
        regardless of the verbose setting.

    device : str, torch.device, default="cpu"
        The compute device to be used. If set to "cuda", data in torch
        tensors will be pushed to cuda tensors before being sent to the
        module. If set to None, then all compute devices will be left
        unmodified.
    """

    def __init__(
            self, group_ids, time_idx, target, max_prediction_length,
            max_encoder_length, time_varying_known_reals,
            time_varying_unknown_reals, static_categoricals,
            cv_split=None, min_encoder_length=None,
            criterion=None, optimizer=None, lr=1e-5, max_epochs=10,
            batch_size=64, callbacks=None, emb_dim=10, hidden_size=16,
            hidden_continuous_size=8, lstm_layers=2, dropout=0.1,
            output_size=1, verbose=1, device='cpu', **kwargs
    ):
        super().__init__(
            module=TFT, dataset=TimeseriesDataset, group_ids=group_ids,
            time_idx=time_idx, target=target,
            max_prediction_length=max_prediction_length,
            max_encoder_length=max_encoder_length,
            time_varying_known_reals=time_varying_known_reals,
            time_varying_unknown_reals=time_varying_unknown_reals,
            static_categoricals=static_categoricals,
            cv_split=cv_split, min_encoder_length=min_encoder_length,
            criterion=criterion, optimizer=optimizer, lr=lr,
            max_epochs=max_epochs, batch_size=batch_size, callbacks=callbacks,
            verbose=verbose, device=device, **kwargs
        )
        self.emb_dim = emb_dim
        self.hidden_size = hidden_size
        self.hidden_continuous_size = hidden_continuous_size
        self.lstm_layers = lstm_layers
        self.dropout = dropout
        self.output_size = output_size
        self._collate_fn = TFTCollateFn
        self._output_decoder = Seq2SeqOutputDecoder


class TFTCollateFn(BaseCollateFn):
    """Collate fn for temporal fusion transformer (TFT).

    This is a modified version of :meth:`TimeSeriesDataset._collate_fn`
    from pytorch_forecasting library to match skorch requirements for data
    delivery.

    Summary of modifications in order to satisfy skorch convention:
    - weights are ignored
    - X is a dict of a dict
    - y is reshaped to 2D
    """

    def __init__(self, dataset):
        super().__init__(dataset)

    def __call__(self, batch):
        batch_size = len(batch)
        X, y = self.dataset._collate_fn(batch)
        y = y[0].reshape(batch_size, -1)
        return {'x': X}, y
