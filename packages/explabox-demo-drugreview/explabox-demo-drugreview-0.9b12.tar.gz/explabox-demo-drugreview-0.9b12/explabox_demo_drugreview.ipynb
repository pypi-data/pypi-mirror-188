{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dbd777c",
   "metadata": {},
   "source": [
    "## Explabox Demo: Drug Review Classification\n",
    "Welcome to the demo of the [explabox](https://explabox.rtfd.io) on the [UCI Drug Reviews](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29#) dataset. To speed up the demo, we made a smaller subset of the train and test dataset. The demo \n",
    "also includes a pretrained black-box classifier, which aims to predict whether a `review` in the text got a `rating` of \n",
    "`negative` (1-5), `neutral` (5-6) or `positive` (6-10).\n",
    "\n",
    "---\n",
    "\n",
    "#### What is the `explabox`?\n",
    "The `explabox` aims to support data scientists and machine learning (ML) engineers in explaining, testing and documenting AI/ML models, developed in-house or acquired externally. The `explabox` turns your **ingestibles** (AI/ML model and/or dataset) into **digestibles** (statistics, explanations or sensitivity insights)!\n",
    "\n",
    "The `explabox` can be used to:\n",
    "\n",
    "- __Explore__: describe aspects of the model and data.\n",
    "- __Examine__: calculate quantitative metrics on how the model performs\n",
    "- __Expose__: see model sensitivity to random inputs (_robustness_), test model generalizability (_robustness_), and see the effect of adjustments of attributes in the inputs (e.g. swapping male pronouns for female pronouns; _fairness_), for the dataset as a whole (_global_) as well as for individual instances (_local_).\n",
    "- __Explain__: use XAI methods for explaining the whole dataset (_global_), model behavior on the dataset (_global_), and specific predictions/decisions (_local_).\n",
    "\n",
    "A number of experiments in the `explabox` can also be used to provide transparency and explanations to stakeholders, such as end-users or clients.\n",
    "\n",
    "&copy; National Police Lab AI (NPAI), 2022\n",
    "\n",
    "#### Need help?\n",
    "The `explabox` documentation at [explabox.rtfd.io](https://explabox.rtfd.io) includes help on installation and usage, and a full API reference.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ab496f",
   "metadata": {},
   "source": [
    "## Let's get started!\n",
    "The demo will walk you through importing your data and model into the `explabox`, and go over some examples for the `explore`, `examine`, `expose` and `explain` parts. The demo is structured as follows:\n",
    "\n",
    "1. **[Ingestibles](#ingestibles)**: importing your model and data.\n",
    "2. **[Explore](#explore)**: exploring the dataset descriptives.\n",
    "3. **[Examine](#examine)**: examine model behavior on the data.\n",
    "4. **[Explain](#explain)**: explain how the model behaves in general (global) and for specific predictions (local).\n",
    "5. **[Expose](#expose)**: find where your model is sensitive to, when it breaks down and when its behavior is unfair.\n",
    "6. **[Challenges](#challenges)**: Challenges to further `explore`, `examine`, `expose` and `explain` your ingestibles and turn them into digestibles.\n",
    "\n",
    "<a id='ingestibles'></a>\n",
    "## 1. Ingestibles\n",
    "The general idea of the `explabox` is to turn your `ingestibles` into `digestibles`. Your `ingestibles` (model and data) are typically opaque and difficult to understand. They require a lot of effort to trust and understand, to test, and communicate the results of these tests. We made that a lot simpler! We have several components that turn the `ingestibles` into `digestibles`: experiments that increase the transparency of your model, and are easy to understand and share.\n",
    "\n",
    "To get started, the `explabox` requires a `model` (e.g. a Scikit-learn classifier or a deep neural network trained in PyTorch) and the `data` (e.g. Huggingface datasets or a Pandas dataframe) you want to turn into `digestibles`. An example of a dataset for debugging purposes is the test set of the model. The `explabox` has two simple functions that import the data and model for you: `explabox.import_data(...)` and `explabox.import_model(...)`. For the demo, we have already provided you with the location of the dataset file (`dataset_file`) and imported the model for you (`model`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8333e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from explabox_demo_drugreview import model, dataset_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13f0755",
   "metadata": {},
   "source": [
    "The [UCI Drug Reviews](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29#) dataset contains over 200,000 patient reviews for drugs. The dataset includes the following columns:\n",
    "\n",
    "Column | Description | Data type\n",
    "-------|-------------|--------\n",
    "`drugName` | Name of drug discussed in review | Categorical (string)\n",
    "`condition` | Name of condition discussed in review | Categorical (string)\n",
    "`review` | Patient review | Free text (string)\n",
    "`rating` | 10 star patient rating | Rating `1-10` (integer)\n",
    "`date` | Date of review entry | Date \n",
    "`usefulCount` | Number of users who found the review useful | Continuous value `>=0` (integer)\n",
    "\n",
    "To speed up the demo, we have included a small subset of examples from the original `train` and `test` set. We made a classifier, that predicts the sentiment for each review: `negative` (1-5), `neutral` (5-6) or `positive` (6-10). To make the performance comparisons easier, the `rating` column in the example dataset has also been transformed to these three categorical values.\n",
    "\n",
    "Let us start by importing the data, where we are interesting in the textual reviews in the `'review'` column and the labels in the `'rating'` column. The `dataset_file` is the location of the dataset (`drugsCom.zip`), containing a train split (`drugsComTrain.tsv`) and test split (`drugsComTest.tsv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08c6051",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d847bfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from explabox import import_data\n",
    "data = import_data(dataset_file, data_cols='review', label_cols='rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8a687f",
   "metadata": {},
   "source": [
    "The `model` included has already been passed through the `model = import_model(...)` function for you, and can therefore be used directly. This is on purpose, so the model is a true black-box for you!\n",
    "\n",
    "> **Now let's explore/examine/expose/explain your model with the Explabox!**\n",
    "\n",
    "Make sure you explicitly include that `drugsComTrain.tsv` includes the train split and `drugsComTest.tsv` the test split of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb934c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from explabox import Explabox\n",
    "\n",
    "box = Explabox(data=data,\n",
    "               model=model,\n",
    "               splits={'train': 'drugsComTrain.tsv', 'test': 'drugsComTest.tsv'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557ece64",
   "metadata": {},
   "source": [
    "Now you are ready to `.explore`, `.examine`, `.expose` and `.explain` with the `explabox`!\n",
    "\n",
    "> *NOTE*: You can use `help(...)` at any time to better understand a model or function.\n",
    "\n",
    "<a id='explore'></a>\n",
    "## 2. Explore\n",
    "The *Explorer* explores your data by providing descriptive statistics. It is included in the Explabox under the `.explore` property.\n",
    "\n",
    "Get descriptives for all splits by calling `box.explore()` or `box.explore.descriptives()`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af18f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "box.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98214c3a",
   "metadata": {},
   "source": [
    "\n",
    "Even though we trust you could have calculated each of these yourselves, it sure saves a lot of work. One simple call and that is all there is. Let's `examine` (see what I did there?) some more impressive functionalities.\n",
    "\n",
    "<a id='examine'></a>\n",
    "## 3. Examine\n",
    "Now we've got a gist of what the data looks like, how does the model perform on the data? Simple, just call `box.examine()` or `box.examine.performance()`. To do so, the _Examiner_ requires a 'model' and 'data'. It is included in the `explabox` under the `.examine` property.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ba8d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "box.examine(split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806a04d3",
   "metadata": {},
   "source": [
    "_That's some magic!_\n",
    "\n",
    "It sure is! The `explabox` inferred your model is a classifier, got all the dataset splits and did all the work for you. It even includes links to explain what all the metrics included mean! Some magic box, right?\n",
    "\n",
    "The `explabox` even allows us to dive deeper into where the model went wrong. Let us see which examples were wrongly classified:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c8b366",
   "metadata": {},
   "outputs": [],
   "source": [
    "box.examine.wrongly_classified()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2157446",
   "metadata": {},
   "source": [
    "<a id='explain'></a>\n",
    "## 4. Explain\n",
    "So what makes the `explabox` so unique? Why not use one of the many other tools for generating dataset descriptives and calculating performance?\n",
    "\n",
    "Well, the `explabox` doesn't stop there! That's just where it starts. Ever heard of _explainable artificial intelligence_ (XAI)? We've included that for you!\n",
    "\n",
    "It doesn't matter if you use the explanations for yourself, show your end-user why a decision was made, to test an externally acquired model, or to provide model clients and supervisory authorities with the insights they require. We can help you on all of those. The explanations included are either _local_ (providing explanations for a single prediction) or _global_ (providing explanations for one or more dataset splits).\n",
    "\n",
    "The _Explainer_ creates explanations corresponding to a model and dataset (with ground-truth labels). The _Explainer_ requires ‘data’ and ‘model’ defined. It is included in the `explabox` under the `.explain` property.\n",
    "\n",
    "### 4.1 Local explanations\n",
    "Why did my model predict a class label? Look no further than `box.explain.explain_prediction(...)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567b7d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "box.explain.explain_prediction('Hate this medicine so much!', method='shap')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a6cc88",
   "metadata": {},
   "source": [
    "_Even more magic!_\n",
    "\n",
    "We've got the work covered for you. Even though it is very easy to use the defaults, you can incorporate your own requirements into the function call:\n",
    "- Want more samples? `.explain_prediction(..., n_instances=500)`\n",
    "- Unweighed samples? `.explain_prediction(..., weigh_samples=False)`\n",
    "- Want other methods? `.explain_prediction(..., methods=['lime', 'shap', 'local_tree'])`\n",
    "\n",
    "So many options to choose from! It uses [text_explainability](https://marcelrobeer.github.io/text_explainability/) for all these methods, which provides a generic architecture for constructing local/global explanation methods. Want to see all options? Check out the [documentation](https://marcelrobeer.github.io/text_explainability/reference/text_explainability/local_explanation/).\n",
    "\n",
    "### 4.2 Global explanations\n",
    "A lot of model behavior can be explained through the data it trained on. So, are there specific tokens corresponding to each label in the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7f92f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "box.explain.token_frequency(splits='train', explain_model=False, labelwise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01bdcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "box.explain.token_information(splits='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548724f0",
   "metadata": {},
   "source": [
    "We could have done the same for the `'test'` split, or explaining model predictions rather than ground-truth labels (`explain_model=True`) or aggregating them over all labels (`labelwise=False`). Want to know how informative tokens are in splitting labels? Try `box.explain.token_information(...)`.\n",
    "\n",
    "The datasets include a lot of examples. Can we summarize them in fewer examples, let's say 5? That's what `box.explain.prototypes(...)` does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c999a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "box.explain.prototypes(n=5, method='kmedoids')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7151f19",
   "metadata": {},
   "source": [
    "Or maybe add some outliers (so-called _criticisms_), that are a-typical for the dataset split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f11ecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "box.explain.prototypes_criticisms(n_prototypes=5, n_criticisms=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c23f4c13",
   "metadata": {},
   "source": [
    "There are so many options for explanations, provided for text datasets by the [text_explainability](https://marcelrobeer.github.io/text_explainability/) package. Check it out to see what is possible!\n",
    "\n",
    "<a id='expose'></a>\n",
    "## 5. Expose\n",
    "Last, but far from least, the _Exposer_ exposes your model and/or data, by performing sensitivity tests. With the _Exposer_ you can see model sensitivity to random inputs (_safety_), test model generalizability (_robustness_), and see the effect of adjustments of attributes in the inputs (e.g. swapping male pronouns for female pronouns; _fairness_), for the dataset as a whole (_global_) as well as for individual instances (_local_).\n",
    "\n",
    "The _Exposer_ requires ‘data’ and ‘model’ defined. It is included in the `explabox` under the `.expose` property.\n",
    "\n",
    "### 5.1 Safety\n",
    "Does your text classifier break down at some inputs? Strings it cannot parse? Try exposing the input space to see its robustness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47014c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "box.expose.input_space('all', min_length=0, max_length=6000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0625ef29",
   "metadata": {},
   "source": [
    "### 5.2 Robustness\n",
    "Or take a global approach by seeing what happens if you transform all instances in the 'test' split from their original form to uppercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c089d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "box.expose.compare_metric(perturbation='upper')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b4f5b82",
   "metadata": {},
   "source": [
    "For both functions there are many techniques to choose from. Why not try exposing the input space with only `'ascii_upper'` and `'whitespace'`? Or try see how introducing `'add_typos'` affects your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9bbc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "box.expose.compare_metric(perturbation='add_typos')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca2e5b7f",
   "metadata": {},
   "source": [
    "### 5.2 Fairness & Robustness: Pattern effects\n",
    "Sometimes you need to go beyond the data to see model robustness and fairness. For the text domain, you can generate data with the [text_sensitivity](https://marcelrobeer.github.io/text_sensitivity/) package and see how the models performs on them.\n",
    "\n",
    "To do so, you write so-called _patterns_ that generate data for you. At spots where you want some data filled in, you simply include curly braces and we fill in the data for you. For some entities (`name`, `city`, `email`, `year`, `month`, ...) we can even generate the data for you. Patterns with a pipe (`|`) simply put in the values you provided. Under the hood, it uses `from_pattern(...)` in the [text_sensitivity](https://marcelrobeer.github.io/text_sensitivity/example_usage/#using-text-sensitivity) package. Example patterns include:\n",
    "\n",
    "Pattern | Generates\n",
    "--------|------------\n",
    "`from_pattern('My phone number is {phone_number}')` | 'My phone number is 001-364-187-2809', 'My phone number is +1-099-759-8699', ...\n",
    "`from_pattern('{upper:name} is from {city}.)'` | 'JAMES RUSSEL is from Oklahoma City.', 'BRIAN WILSON is from Millermouth.', ...\n",
    "`from_pattern('{His\\|Her} favorite girl is {female_name}', female_name=RandomFirstName(sex='female'))` | 'His favorite girl is Julia', 'Her favorite girl is Julia', ...\n",
    "\n",
    "Let's turn that generated data into a proper test, where we except that the review is `positive` regardless:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635215a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "box.expose.invariance('My friend {name} {loves|likes} this medicine. It is amazing!',\n",
    "                      expectation='positive',\n",
    "                      n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042347bb",
   "metadata": {},
   "source": [
    "Or one where it is `negative` regardless:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0306db",
   "metadata": {},
   "outputs": [],
   "source": [
    "box.expose.invariance('My friend {upper:first_name} {hates|dislikes} this medicine. It is{| not} terrible!',\n",
    "                      expectation='negative',\n",
    "                      n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b3cc5",
   "metadata": {},
   "source": [
    "Or simply output the mean probabilistic score for the label `negative` for the generated instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f956a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "box.expose.mean_score('My friend {first_name} from {city} {hates|dislikes} this medicine!',\n",
    "                      selected_labels='negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1e660d",
   "metadata": {},
   "source": [
    "In the near future, the package will also include functionalities for calculating fairness metrics on protected groups.\n",
    "\n",
    "<a id='challenges'></a>\n",
    "## 6. Challenges\n",
    "Want some pointers on where to go to next? Want to further `.explore`, `.examine`, `.expose` and `.explain` the black-box we provided? We've got some fun ideas to try out for yourself! Be sure to use the [API Reference](https://explabox.readthedocs.io/en/latest/api/explabox.html) to figure out how to do them.\n",
    "\n",
    "##### A. Which tokens are the most informative in distinguishing between the predicted classes?\n",
    "<details>\n",
    "    <summary>Show hints</summary>\n",
    "    Try `help(box.explain.token_information)`\n",
    "</details>\n",
    "\n",
    "###### B. Can I globally change the language to Dutch ('nl') if my data is Dutch?\n",
    "<details>\n",
    "    <summary>Show hints</summary>\n",
    "    See `help(Explabox)`\n",
    "</details>\n",
    "\n",
    "##### C. How do local explanations with LIME compare against scores with KernelSHAP?\n",
    "<details>\n",
    "    <summary>Show hints</summary>\n",
    "    Try `box.explain.explain_prediction(..., methods=['lime', 'kernelshap'])`\n",
    "</details>\n",
    "\n",
    "##### D. What happens if you change the `l1 regularization` of KernelSHAP?\n",
    "<details>\n",
    "    <summary>Show hints</summary>\n",
    "    Try `box.explain.explain_prediction(..., methods=['kernelshap'], l1_reg=...)`\n",
    "</details>\n",
    "\n",
    "##### E. How does the model perform if you repeat each sentence in the test set twice?\n",
    "<details>\n",
    "    <summary>Show hints</summary>\n",
    "    Try `box.expose.compare_metrics(perturbation='repeat')`\n",
    "</details>\n",
    "\n",
    "##### F. How does the model perform if you start each review with 'This is a review.'?\n",
    "<details>\n",
    "    <summary>Show hints</summary>\n",
    "    Try \n",
    "```python\n",
    "from explabox.expose import OneToOnePerturbation\n",
    "perturbation = OneToOnePerturbation(lambda x: f'This is a review. {x}')\n",
    "box.expose.compare_metrics(perturbation=perturbation)\n",
    "```\n",
    "</details>\n",
    "\n",
    "##### G. Does adding random typos degrade model performance?\n",
    "<details>\n",
    "    <summary>Show hints</summary>\n",
    "    Try \n",
    "```python\n",
    "from explabox.expose.text import OneToOnePerturbation\n",
    "perturbation = OneToOnePerturbation(lambda x: f'{x}!!')\n",
    "box.expose.compare_metrics(perturbation=perturbation)\n",
    "```\n",
    "</details>\n",
    "\n",
    "##### H. Are there any drug names (https://www.drugs.com/drug_information.html) that seem to have more positive scores?\n",
    "<details>\n",
    "    <summary>Show hints</summary>\n",
    "    Try `box.expose.mean_score('{drug}', drug=['Acetaminophen', 'Adderral', ...], selected_labels='positive')`\n",
    "</details>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
